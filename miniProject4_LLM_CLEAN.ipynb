{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbpMsByRZFff"
   },
   "source": [
    "<center><font size=6>Large Langauge Models (LLMs) and Retrieval Augmented Generation (RAG)</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4BFwWexZg7l"
   },
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SyZwqY4Zkwh"
   },
   "source": [
    "## Business Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuIUZTvjZnVo"
   },
   "source": [
    "Growing organizations generate massive amounts of reports and data critical for decision-making. For example, venture capital analysts at firms like Andreessen Horowitz must extract insights from dense documents such as HBR’s “How Apple is Organized for Innovation.” Manual review is slow and inefficient, but Semantic Search and Retrieval-Augmented Generation (RAG) can deliver quick, precise answers to targeted questions, enabling faster strategic insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zkx52_ieZqOA"
   },
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aCefUoxZtdS"
   },
   "source": [
    "Build a RAG application that allows business analysts to quickly extract key insights from lengthy reports, improving efficiency and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbMYgzsDZwB5"
   },
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx0FHHmQZzJF"
   },
   "source": [
    "**How Apple is Organized for Innovation** - An article of 11 pages in pdf format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMcFY5CLZ2F4"
   },
   "source": [
    "# Installing and Importing Necessary Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "YaY4TYcUZDq0",
    "outputId": "ac988932-fe7e-4fc7-e67e-2fe8da2e44b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Using cached numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.3 which is incompatible.\n",
      "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.3 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.3 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.3.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "d4312f2ee2a442a799f03471b6049017",
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note- After running this cell, restart session and DONT run this again\n",
    "!pip install --upgrade --force-reinstall numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yd5Zn7WeSGXh",
    "outputId": "63af086f-b18a-451d-92d7-fd3bc3246f3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# Installation for GPU llama-cpp-python\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.28  --force-reinstall --upgrade --no-cache-dir -q 2>/dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zEiDvPpwSJ0F",
    "outputId": "fe02ad2f-6bd0-4c5f-affc-2b062fc9f262"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-6.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-1.1.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.35.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.77)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.31)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Collecting requests>=2.26.0 (from tiktoken)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.3.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
      "Collecting pybase64>=1.4.1 (from chromadb)\n",
      "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.37.0)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.75.1)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.19.2)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.10)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading pypdf-6.1.1-py3-none-any.whl (323 kB)\n",
      "Downloading langchain_community-0.3.30-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading chromadb-1.1.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m171.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
      "Downloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading onnxruntime-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl (19 kB)\n",
      "Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
      "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
      "Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m153.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=0c833e2050f3f11186d0e2643f8a5694560a55814d91b4552d8a98e5527108d1\n",
      "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, durationpy, uvloop, urllib3, pypdf, pybase64, mypy-extensions, mmh3, marshmallow, humanfriendly, httptools, bcrypt, backoff, watchfiles, typing-inspect, requests, coloredlogs, posthog, onnxruntime, dataclasses-json, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb, langchain-community\n",
      "\u001b[2K  Attempting uninstall: urllib3\n",
      "\u001b[2K    Found existing installation: urllib3 2.5.0\n",
      "\u001b[2K    Uninstalling urllib3-2.5.0:\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.5.0\n",
      "\u001b[2K  Attempting uninstall: requests\n",
      "\u001b[2K    Found existing installation: requests 2.32.4\n",
      "\u001b[2K    Uninstalling requests-2.32.4:\n",
      "\u001b[2K      Successfully uninstalled requests-2.32.4\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/24\u001b[0m [langchain-community]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 chromadb-1.1.1 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-34.1.0 langchain-community-0.3.30 marshmallow-3.26.1 mmh3-5.2.0 mypy-extensions-1.1.0 onnxruntime-1.23.0 opentelemetry-exporter-otlp-proto-grpc-1.37.0 posthog-5.4.0 pybase64-1.4.2 pypdf-6.1.1 pypika-0.48.9 requests-2.32.5 typing-inspect-0.9.0 urllib3-2.3.0 uvloop-0.21.0 watchfiles-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken pypdf langchain langchain-community chromadb sentence-transformers huggingface_hub\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XPmEWmruSQ2D"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader, PyPDFLoader\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from google.colab import userdata, drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXG4cKvtaebo"
   },
   "source": [
    "# Data Preparation for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egj4GVFeagFu"
   },
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BKYp8xeISUES"
   },
   "outputs": [],
   "source": [
    "apple_pdf_path = \"/content/apple.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LrndgfUHTEi3"
   },
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(apple_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VARNz6P9Tdrk"
   },
   "outputs": [],
   "source": [
    "apple = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38y_WR4HanEQ"
   },
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbfDz7hban02"
   },
   "source": [
    "#### Checking the first 3 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0E39ZKrTmAO",
    "outputId": "41736a55-bf4c-41ac-d13a-c37e262cd97a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Number : 1\n",
      "REPRINT R2006F\n",
      "PUBLISHED IN HBR\n",
      "NOVEMBER–DECEMBER 2020\n",
      "ARTICLEORGANIZATIONAL CULTURE\n",
      "How Apple Is \n",
      "Organized  \n",
      "for Innovation\n",
      "It’s about experts leading experts. \n",
      "by Joel M. Podolny and Morten T. Hansen\n",
      "This article is made available to you with compliments of Apple Inc for your personal use. Further posting, copying or distribution is not permitted.\n",
      "Page Number : 2\n",
      "2\n",
      "Harvard Business Review\n",
      "November–December 2020\n",
      "This article is made available to you with compliments of Apple Inc for your personal use. Further posting, copying or distribution is not permitted.\n",
      "Page Number : 3\n",
      "PHOTOGRAPHER MIKAEL JANSSON\n",
      "How Apple Is  Organized  for InnovationIt’s about experts  leading experts.\n",
      "ORGANIZATIONAL \n",
      "CULTURE\n",
      "Joel M. \n",
      "Podolny\n",
      "Dean, Apple \n",
      "University\n",
      "Morten T. \n",
      "Hansen\n",
      "Faculty, Apple \n",
      "University\n",
      "AUTHORS\n",
      "FOR ARTICLE REPRINTS CALL 800-988-0886 OR 617-783-7500, OR VISIT HBR.ORG\n",
      "Harvard Business Review\n",
      "November–December 2020  3\n",
      "This article is made available to you with compliments of Apple Inc for your personal use. Further posting, copying or distribution is not permitted.\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Page Number : {i+1}\",end=\"\\n\")\n",
    "    print(apple[i].page_content,end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "LtKx2MArTow5",
    "outputId": "e61ca203-22a0-4918-d6f8-cffbf42fc1d0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'targets were the overriding criteria for judging investments \\nand leaders. Significantly, the bonuses of senior R&D exec-\\nutives are based on companywide performance numbers \\nrather than the costs of or revenue from particular products. \\nThus product decisions are somewhat insulated from short-\\nterm financial pressures. The finance team is not involved in \\nthe product road map meetings of engineering teams, and \\nengineering teams are not involved in pricing decisions.\\nWe don’t mean to suggest that Apple doesn’t consider \\ncosts and revenue goals when deciding which technologies \\nand features the company will pursue. It does, but in ways \\nthat differ from those employed by conventionally organized \\ncompanies. Instead of using overall cost and price targets as \\nfixed parameters within which to make design and engineer-\\ning choices, R&D leaders are expected to weigh the benefits \\nto users of those choices against cost considerations.\\nIn a functional organization, individual and team repu-\\ntations act as a control mechanism in placing bets. A case in \\npoint is the decision to introduce the dual-lens camera with \\nportrait mode in the iPhone 7 Plus in 2016. It was a big wager \\nthat the camera’s impact on users would be sufficiently great \\nto justify its significant cost.\\nOne executive told us that Paul Hubel, a senior leader \\nwho played a central role in the portrait mode effort, was \\n“out over his skis, ” meaning that he and his team were taking \\na big risk: If users were unwilling to pay a premium for a \\nphone with a more costly and better camera, the team would \\nmost likely have less credibility the next time it proposed an \\nexpensive upgrade or feature. The camera turned out to be a \\ndefining feature for the iPhone 7 Plus, and its success further \\nenhanced the reputations of Hubel and his team.\\nIt’s easier to get the balance right between an attention to \\ncosts and the value added to the user experience when the \\nleaders making decisions are those with deep expertise in \\ntheir areas rather than general managers being held account-\\nable primarily for meeting numerical targets. Whereas the \\nfundamental principle of a conventional business unit struc-\\nture is to align accountability and control, the fundamental \\nprinciple of a functional organization is to align expertise and \\ndecision rights.\\nThus the link between how Apple is organized and \\nthe type of innovations it produces is clear. As Chandler \\nfamously argued, “structure follows strategy”—even though \\nApple doesn’t use the structure that he anticipated large \\nmultinationals would adopt.\\nNow let’s turn to the leadership model underlying Apple’s \\nstructure.\\nTHREE LEADERSHIP CHARACTERISTICS\\nEver since Steve Jobs implemented the functional organi-\\nzation, Apple’s managers at every level, from senior vice \\npresident on down, have been expected to possess three key \\nleadership characteristics: deep expertise that allows them \\nto meaningfully engage in all the work being done within \\ntheir individual functions; immersion in the details of those \\nfunctions; and a willingness to collaboratively debate other \\nfunctions during collective decision-making. When manag-\\ners have these attri\\n but\\nes, decisions are made in a coordinated \\nfashion by the people most qualified to make them.\\nDeep expertise. Apple is not a company where general \\nmanagers oversee managers; rather, it is a company where \\nexperts lead experts. The assumption is that it’s easier to \\ntrain an expert to manage well than to train a manager to be \\nan expert. At Apple, hardware experts manage hardware, \\nsoftware experts software, and so on. (Deviations from \\nthis principle are rare.) This approach cascades down all \\nlevels of the organization through areas of ever-\\n inc\\nreasing \\n2019\\nDESIGN\\nOPERATIONS\\nSALES\\nHARDWARE  \\nENGINEERING\\nRETAIL\\nPEOPLE\\nSOFTWARE\\nFINANCE\\nSERVICES\\nLEGAL\\nMACHINE LEARNING & AI\\nCORPORATE COMM.\\nHARDWARE\\nSOFTWARE\\nMARKETING\\nOPERATIONS\\nSERVICES & SUPPORT\\nSALES\\nFINANCE\\nLEGAL\\nMARKETING\\nENVIRONMENT,  \\nPOLICY & SOCIAL\\nHARDWARE  \\nTECHNOLOGIES\\nMARKETING COMM.\\nCORPORATE DEV.\\nCEO\\nApple’s Functional Organization\\nIn 1997, when Steve Jobs returned to Apple, it had a conventional structure for its size and scope. It was divided into business units, each with \\nits own P&L responsibilities. After retaking the helm, Jobs put the entire company under one P&L and combined the disparate departments of \\nthe business units into one functional organization that aligns expertise with decision rights—a structure Apple retains to this day.\\nCEO1998\\n6\\nHarvard Business Review\\nNovember–December 2020\\nThis article is made available to you with compliments of Apple Inc for your personal use. Further posting, copying or distribution is not permitted.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple[5].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBqQzewWcCgx"
   },
   "source": [
    "* If we observe the text closely, the text is not extracted sequentially.  \n",
    "\n",
    "* This is a limitation, as we are missing coherent text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_m8oaDhcG26"
   },
   "source": [
    "#### Checking the number of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvAIySN-Ts4b",
    "outputId": "ce6a8cfd-3bbe-4372-fada-a870bdea5ddb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(apple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfSmY5AncMtG"
   },
   "source": [
    "## Data Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Yc839xjRVz32"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name='cl100k_base',\n",
    "    chunk_size=512,\n",
    "    chunk_overlap= 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dUPEViHnV3rq"
   },
   "outputs": [],
   "source": [
    "document_chunks = pdf_loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_zpH8LNV6fS",
    "outputId": "4afb8db2-eb8b-4221-f29c-982c951b5f2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "TJK2SY54V96D",
    "outputId": "0a4475cb-0fdb-4a23-ecbd-15bbf15f499b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'REPRINT R2006F\\nPUBLISHED IN HBR\\nNOVEMBER–DECEMBER 2020\\nARTICLEORGANIZATIONAL CULTURE\\nHow Apple Is \\nOrganized  \\nfor Innovation\\nIt’s about experts leading experts. \\nby Joel M. Podolny and Morten T. Hansen\\nThis article is made available to you with compliments of Apple Inc for your personal use. Further posting, copying or distribution is not permitted.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chunks[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "b5oPFiNK5PtT",
    "outputId": "fba0e5d9-f406-4b15-d243-9c5897231a8f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'the answer (because they don’t). This differs starkly from \\nthe way leaders question subordinates about activities in the \\nowning and teaching boxes.\\nFinally, Rosner has delegated some areas—including \\niMovie and GarageBand, in which he is not an expert—to \\npeople with the requisite capabilities. For activities in the \\ndelegating box, he assembles teams, agrees on objectives, \\nmonitors and reviews prog ress, and holds the teams account-\\nable: the stuff of general management.\\nWhereas Apple’s VPs spend most of their time in the own-\\ning and learning boxes, general managers at other companies \\ntend to spend most of their time in the delegating box. Rosner \\nestimates that he spends about 40% of his time on activities \\nhe owns (including collaboration with others in a given area), \\nabout 30% on learning, about 15% on teaching, and about 15% \\non delegating. These numbers vary by manager, of course, \\ndepending on their business and the needs at a given time.\\nThe discretionary leadership model preserves the funda-\\nmental principle of an effective functional organization at \\nscale—aligning expertise and decision rights. Apple can \\neffectively move into new areas when leaders like Rosner \\ntake on new responsibilities outside their original expertise, \\nand teams can grow in size when leaders teach others their \\ncraft and delegate work. We believe that Apple will continue \\nto innovate and prosper by being organized this way.\\nAPPLE’S FUNCTIONAL ORGANIZATION is rare, if not unique, \\namong very large companies. It flies in the face of prevailing \\nmanagement theory that companies should be reorganized \\ninto divisions and business units as they become large. But \\nsomething vital gets lost in a shift to business units: the \\nalignment of decision rights with expertise.\\nWhy do companies so often cling to having general man-\\nagers in charge of business units? One reason, we believe, \\nis that making the change is difficult. It entails overcoming \\ninertia, reallocating power among managers, changing an \\nindividual- oriented incentive system, and learning new ways \\nof collaborating. That is daunting when a company already \\nfaces huge external challenges. An intermediate step may be \\nto cultivate the experts-leading-experts model even within'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chunks[-2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "f7vZlk9T58_K",
    "outputId": "8cf0402f-dee9-42eb-ecce-b43b40eb68fa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'to cultivate the experts-leading-experts model even within \\na business unit structure. For example, when filling the next \\nsenior management role, pick someone with deep expertise \\nin that area as opposed to someone who might make the best \\ngeneral manager. But a full-fledged transformation requires \\nthat leaders also transition to a functional organization. \\nApple’s track rec ord proves that the rewards may justify the \\nrisks. Its approach can produce extraordinary results. \\nHBR Reprint R2006F\\nJOEL M. PODOLNY  is a vice president of Apple and the dean  \\nof Apple University. Prior to joining Apple, in 2009, he was  \\nthe dean of the Yale School of Management and on the faculty of \\nHarvard’s and Stanford’s business schools. MORTEN T. HANSEN  \\nis a member of Apple University’s faculty and a professor at the \\nUniversity of California, Berkeley. He was formerly on the faculties  \\nof Harvard Business School and INSEAD.\\nFOR ARTICLE REPRINTS CALL 800-988-0886 OR 617-783-7500, OR VISIT HBR.ORG\\nHarvard Business Review\\nNovember–December 2020 \\u200911\\nThis article is made available to you with compliments of Apple Inc for your personal use. Further posting, copying or distribution is not permitted.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chunks[-1].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByPGtUykcUrz"
   },
   "source": [
    "As expected, there are some overlaps:  \n",
    "\n",
    "- The sentence '*to cultivate the experts-leading-experts model even within*' appears in both chunks.  \n",
    "- If we increase the `chunk_overlap`, the overlapping length of the sentence will also increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsS7mpMTcWq3"
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "id": "ZxFTW9Hx6AVM",
    "outputId": "3c4d1618-f01b-4032-8f14-610728e7675b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-4198310515.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = SentenceTransformerEmbeddings(model_name='thenlper/gte-large')\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a4cfbd38ac4cc09c0c51506fef4b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a659c4557a4b9fb98cf39d1090308d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6a80f96fd04f2899094b2075aa3061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d287f070b0e4e55b7e95ae7373229ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d578b823c4004384b580939888f0599b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/670M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c18c25e922a451fa7f4e292ad57a698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd819d5c634a40d58cdd47ccd31f97ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fac94a4565645509cf8bce5b7add0c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f669b28c948649a18bb819c868efac3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec20dcc77f564b62b19454748374debe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_model = SentenceTransformerEmbeddings(model_name='thenlper/gte-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "d7IuS4FG-AQu"
   },
   "outputs": [],
   "source": [
    "embedding_1 = embedding_model.embed_query(document_chunks[0].page_content)\n",
    "embedding_2 = embedding_model.embed_query(document_chunks[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVQUESkG-Ey6",
    "outputId": "a41544a8-31a3-42e5-88dc-e3076b2915c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the embedding vector  1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dimension of the embedding vector \",len(embedding_1))\n",
    "len(embedding_1)==len(embedding_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZ6HCeGpcrBt"
   },
   "source": [
    "* The embedding model provides a fixed-length vector for any number of chunks.  \n",
    "* This is necessary because we want to compare them for similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnxYJ86icutp"
   },
   "source": [
    "## Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "TWsGpQkq1brk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "out_dir = 'apple_db'\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "  os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "7TPwSJ6m-JUX"
   },
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    document_chunks,\n",
    "    embedding_model,\n",
    "    persist_directory=out_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KmBQyy_02RoX",
    "outputId": "f8ff61ab-117e-4df4-ba54-8b82be6af6d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2756559696.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=out_dir,embedding_function=embedding_model)\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma(persist_directory=out_dir,embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i9mm7nUw2UVR",
    "outputId": "cde8abc1-454c-4a42-e48a-6e7027c6698a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
       "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='thenlper/gte-large', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g44tAIyq3Qem",
    "outputId": "d03db1e6-c4ec-4004-f9cd-d427a1aeb57a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page_label': '5', 'creationdate': '2020-10-05T14:18:42-04:00', 'source': '/content/apple.pdf', 'producer': 'Adobe PDF Library 15.0 (via http://bfo.com/products/pdf?version=2.23.5-r33279)', 'page': 4, 'trapped': '/False', 'creator': 'Adobe InDesign 14.0 (Macintosh)', 'total_pages': 11, 'moddate': '2020-12-01T18:37:49+00:00'}, page_content='WHY A FUNCTIONAL ORGANIZATION?\\nApple’s main purpose is to create products that enrich \\npeople’s daily lives. That involves not only developing \\nentirely new product categories such as the iPhone and the \\nApple Watch, but also continually innovating within those \\ncategories. Perhaps no product feature better reflects Apple’s \\ncommitment to continuous innovation than the iPhone cam-\\nera. When the iPhone was introduced, in 2007, Steve Jobs \\ndevoted only six seconds to its camera in the annual keynote \\nevent for unveiling new products. Since then iPhone camera \\ntechnology has contributed to the photography industry \\nwith a stream of innovations: High dynamic range imaging \\n(2010), panorama photos (2012), True Tone flash (2013), opti-\\ncal image stabilization (2015), the dual-lens camera (2016), \\nportrait mode (2016), portrait lighting (2017), and night mode \\n(2019) are but a few of the improvements.\\nTo create such innovations, Apple relies on a structure \\nthat centers on functional expertise. Its fundamental belief \\nis that those with the most expertise and experience in a \\ndomain should have decision rights for that domain. This \\nis based on two views: First, Apple competes in markets \\nwhere the rates of technological change and disruption are \\nhigh, so it must rely on the judgment and intuition of people \\nwith deep knowledge of the technologies responsible for \\ndisruption. Long before it can get market feedback and solid \\nmarket forecasts, the company must make bets about which \\ntechnologies and designs are likely to succeed in smart-\\nphones, computers, and so on. Relying on technical experts \\nrather than general managers increases the odds that those \\nbets will pay off.\\nSecond, Apple’s commitment to offer the best possible \\nproducts would be undercut if short-term profit and cost \\nABOUT THE ART\\nApple Park, Apple’s corporate headquarters in  \\nCupertino, California, opened in 2017.\\nMikael Jansson/Trunk Archive\\nFOR ARTICLE REPRINTS CALL 800-988-0886 OR 617-783-7500, OR VISIT HBR.ORG\\nHarvard Business Review\\nNovember–December 2020 \\u20095'),\n",
       " Document(metadata={'creationdate': '2020-10-05T14:18:42-04:00', 'producer': 'Adobe PDF Library 15.0 (via http://bfo.com/products/pdf?version=2.23.5-r33279)', 'trapped': '/False', 'source': '/content/apple.pdf', 'total_pages': 11, 'page': 3, 'moddate': '2020-12-01T18:37:49+00:00', 'creator': 'Adobe InDesign 14.0 (Macintosh)', 'page_label': '4'}, page_content='WELL KNOWN FOR ITS innovations in hardware, software, \\nand services. Thanks to them, it grew from some 8,000 \\nemployees and $7\\n \\nbillion in revenue in 1997, the year Steve \\nJobs returned, to 137,000 employees and $260\\n \\nbillion in \\nrevenue in 2019. Much less well known are the organizational \\ndesign and the associated leadership model that have played \\na crucial role in the company’s innovation success.\\nWhen Jobs arrived back at Apple, it had a conventional \\nstructure for a company of its size and scope. It was divided \\ninto business units, each with its own P&L responsibilities. \\nGeneral managers ran the Macintosh products group, the \\ninformation appliances division, and the server products \\ndivision, among others. As is often the case with decentral-\\nized business units, managers were inclined to fight with \\none another, over transfer prices in particular. Believing that \\nconventional management had stifled innovation, Jobs, in \\nhis first year returning as CEO, laid off the general managers \\nof all the business units (in a single day), put the entire com-\\npany under one P&L, and combined the disparate functional \\ndepartments of the business units into one functional organi-\\nzation. (See the exhibit “ Apple’s Functional Organization. ”)\\nThe adoption of a functional structure may have been \\nun\\n surprisin\\ng for a company of Apple’s size at the time. What is \\nsurprising—in fact, remarkable—is that Apple retains it today, \\neven though the company is nearly 40 times as large in terms \\nof revenue and far more complex than it was in 1998. Senior \\nvice presidents are in charge of functions, not products. As \\nwas the case with Jobs before him, CEO Tim Cook occupies the \\nonly position on the organizational chart where the design, \\nengineering, operations, marketing, and retail of any of Apple’s \\nmain products meet. In effect, besides the CEO, the company \\noperates with no conventional general managers: people \\nwho control an entire process from product development \\nthrough sales and are judged according to a P&L statement.\\nBusiness history and organizational theory make the case \\nthat as entrepreneurial firms grow large and complex, they'),\n",
       " Document(metadata={'page': 8, 'creator': 'Adobe InDesign 14.0 (Macintosh)', 'creationdate': '2020-10-05T14:18:42-04:00', 'moddate': '2020-12-01T18:37:49+00:00', 'trapped': '/False', 'producer': 'Adobe PDF Library 15.0 (via http://bfo.com/products/pdf?version=2.23.5-r33279)', 'page_label': '9', 'source': '/content/apple.pdf', 'total_pages': 11}, page_content='things, that these photos often had blurring at the edges of a \\nface but sharpness on the eyes. So they charged the algorithm \\nteams with achieving the same effect. When the teams suc-\\nceeded, they knew they had an acceptable standard.\\nAnother issue that emerged was the ability to preview a \\nportrait photo with a blurred background. The camera team \\nhad designed the feature so that users could see its effect on \\ntheir photos only after they had been taken, but the human \\ninterface (HI) design team pushed back, insisting that users \\nshould be able to see a “live preview” and get some guidance \\nabout how to make adjustments before taking the photo. \\nJohnnie Manzari, a member of the HI team, gave the camera \\nteam a demo. “When we saw the demo, we realized that this \\nis what we needed to do, ” Townsend told us. The members \\nof his camera hardware team weren’t sure they could do \\nit, but difficulty was not an acceptable excuse for failing to \\ndeliver what would clearly be a superior user experience. After \\nmonths of engineering effort, a key stakeholder, the video \\nengineering team (responsible for the low-level software that \\ncontrols sensor and camera operations) found a way, and the \\ncollaboration paid off. Portrait mode was central to Apple’s \\nmarketing of the iPhone 7 Plus. It proved a major reason for \\nusers’ choosing to buy and delighting in the use of the phone.\\nAs this example shows, Apple’s collaborative debate \\ninvolves people from various functions who disagree, push \\nback, promote or reject ideas, and build on one another’s \\nideas to come up with the best solutions. It requires open-\\n \\nmindedness fr\\nom senior leaders. It also requires those \\nleaders to inspire, prod, or influence colleagues in other  \\nareas to contribute toward achieving their goals.\\nWhile Townsend is accountable for how great the camera \\nis, he needed dozens of other teams—each of which had a \\nlong list of its own commitments—to contribute their time and \\neffort to the portrait mode proj\\n ect. A\\nt Apple that’s known as \\naccountability without control: You’re accountable for making \\nthe proj\\n ect succeed ev\\nen though you don’t control all the other')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"Apple Steve Jobs iPhone \",k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnqV6H5SdHFi"
   },
   "source": [
    "* From the retrieved chunks, we observe that all the chunks are related to the key terms [ 'Apple', 'Steve Jobs', 'iPhone' ]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKNHLcJXdKMv"
   },
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "7p-FNhch3UU0"
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type='similarity',\n",
    "    search_kwargs={'k': 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQZD4Xj43Zs2",
    "outputId": "088f12b9-b1b2-4b82-c8ae-f29b7514e1c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3586710401.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  rel_docs = retriever.get_relevant_documents(\"How does does Apple develop and ship products that requires good coordination between the teams?\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'creationdate': '2020-10-05T14:18:42-04:00', 'moddate': '2020-12-01T18:37:49+00:00', 'producer': 'Adobe PDF Library 15.0 (via http://bfo.com/products/pdf?version=2.23.5-r33279)', 'creator': 'Adobe InDesign 14.0 (Macintosh)', 'source': '/content/apple.pdf', 'trapped': '/False', 'total_pages': 11, 'page_label': '8', 'page': 7}, page_content='40 specialist teams: silicon design, camera software, reliabil-\\nity engineering, motion sensor hardware, video engineering, \\ncore motion, and camera sensor design, to name just a few. \\nHow on earth does Apple develop and ship products that \\nrequire such coordination? The answer is collaborative \\ndebate. Because no function is responsible for a product or a \\nservice on its own, cross-functional collaboration is crucial.\\nWhen debates reach an impasse, as some inevitably do, \\nhigher-level managers weigh in as tiebreakers, including at \\ntimes the CEO and the senior VPs. To do this at speed with \\nsufficient attention to detail is challenging for even the best \\nof leaders, making it all the more important that the company \\nfill many senior positions from within the ranks of its VPs, \\nwho have experience in Apple’s way of operating.\\nHowever, given Apple’s size and scope, even the executive \\nteam can resolve only a limited number of stalemates. The \\nmany horizontal dependencies mean that ineffective peer \\nrelationships at the VP and director levels have the potential \\nto undermine not only particular proj\\n ec\\nts but the entire \\ncompany. Consequently, for people to attain and remain in \\na leadership position within a function, they must be highly \\neffective collaborators.\\nThat doesn’t mean people can’t express their points of \\nview. Leaders are expected to hold strong, well-grounded \\nviews and advocate forcefully for them, yet also be willing  \\nto change their minds when presented with evidence \\nthat others’ views are better. Doing so is not always \\neasy, of course. A leader’s ability to be both partisan and \\nopen-minded is facilitated by two things: deep understand-\\ning of and devotion to the company’s values and common \\npurpose, and a commitment to separating how right from \\nhow hard a particular path is so that the difficulty of execut-\\ning a decision doesn’t prevent its being selected.\\nThe development of the iPhone’s portrait mode illustrates \\na fanatical attention to detail at the leadership level, intense \\ncollaborative debate among teams, and the power of a shared \\npurpose to shape and ultimately resolve debates. In 2009 \\nHubel had the idea of developing an iPhone feature that \\nwould allow people to take portrait photos with bokeh—'),\n",
       " Document(metadata={'creationdate': '2020-10-05T14:18:42-04:00', 'producer': 'Adobe PDF Library 15.0 (via http://bfo.com/products/pdf?version=2.23.5-r33279)', 'page': 6, 'creator': 'Adobe InDesign 14.0 (Macintosh)', 'moddate': '2020-12-01T18:37:49+00:00', 'page_label': '7', 'trapped': '/False', 'source': '/content/apple.pdf', 'total_pages': 11}, page_content='Apple is run. Leaders can push, probe, and “smell” an issue. \\nThey know which details are important and where to focus \\ntheir attention. Many people at Apple see it as liberating, \\neven exhilarating, to work for experts, who provide better \\nguidance and mentoring than a general manager would. \\nTogether, all can strive to do the best work of their lives in \\ntheir chosen area.\\nWillingness to collaboratively debate. Apple has \\nhundreds of specialist teams across the company, dozens of \\nwhich may be needed for even one key component of a new \\nproduct offering. For example, the dual-lens camera with \\nportrait mode required the collaboration of no fewer than  \\nApple leaders are expected to possess deep expertise, be immersed \\nin the details of their functions, and engage in collaborative debate.\\nORGANIZATIONAL \\nCULTURE\\nFOR ARTICLE REPRINTS CALL 800-988-0886 OR 617-783-7500, OR VISIT HBR.ORG\\nHarvard Business Review\\nNovember–December 2020 \\u20097\\nThis article is made available to you with compliments of Apple Inc for your personal use. Further posting, copying or distribution is not permitted.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_docs = retriever.get_relevant_documents(\"How does does Apple develop and ship products that requires good coordination between the teams?\")\n",
    "rel_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrTXk--sdT6X"
   },
   "source": [
    "- We can observe that the two relevant chunks contain the answer to the query.  \n",
    "- If we increase the **`k`** value, there is a chance that we might find the answer in even more chunks.  \n",
    "- This is a hyperparameter that we need to tune to get the best context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd39DaTgjD1m"
   },
   "source": [
    "# Defining the Response Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhJeGKqKjzvb"
   },
   "source": [
    "## Downloading and Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5X_87qtj6Ic"
   },
   "source": [
    "### Popular Model Loaders (besides llama-cpp-python)\n",
    "* llama-cpp-python → CPU + GPU, lightweight, best for quantized GGUF models, works even on laptops.\n",
    "\n",
    "* vLLM / TGI → High-throughput GPU inference for production servers.\n",
    "\n",
    "* Transformers → Most flexible, best for research + training + fine-tuning.\n",
    "\n",
    "* Ollama → Best for easy local use without coding.\n",
    "\n",
    "* mlc-llm → Best for edge devices and WebGPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Ufe5dz3z3cUk"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "yVLgcXMuis_7"
   },
   "outputs": [],
   "source": [
    "# Using mistral\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
    "model_basename = \"mistral-7b-instruct-v0.2.Q6_K.gguf\"\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=model_name_or_path,\n",
    "    filename=model_basename\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be1OCxnnkEmT"
   },
   "source": [
    "Below code initializes your Mistral model via llama-cpp-python, sets:\n",
    "\n",
    "- Where the model file is\n",
    "\n",
    "- How many tokens it can handle at once (n_ctx)\n",
    "\n",
    "- How to distribute layers between GPU/CPU (n_gpu_layers)\n",
    "\n",
    "- How many tokens to process at a time (n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dM9bdYnIj-r7",
    "outputId": "6f0a9da2-7631-4d70-ef36-db220d94615a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=2300,\n",
    "    n_gpu_layers=38,\n",
    "    n_batch=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NELB6N3zkMmE",
    "outputId": "a1c13f4c-af9c-465c-c3e9-f149fe365590"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\nApple is known for its ability to design, develop, and ship'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"How does does Apple develop and ship products that requires good coordination between the teams?\")['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jzvv48MnnHr"
   },
   "source": [
    "- The response seems generic and appears to be derived from another article. Let's provide our own context and align the response with our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MN4JrZqanqI8"
   },
   "source": [
    "## System and User Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8ywHz6CnrKz"
   },
   "source": [
    "Prompts guide the model to generate accurate responses. Here, we define two parts:\n",
    "\n",
    "    1. The system message describing the assistant's role.\n",
    "    2. A user message template including context and the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "kupGZXmHkPqH"
   },
   "outputs": [],
   "source": [
    "qna_system_message = \"\"\"\n",
    "You are an assistant whose work is to review the report and provide the appropriate answers from the context.\n",
    "User input will have the context required by you to answer user questions.\n",
    "This context will begin with the token: ###Context.\n",
    "The context contains references to specific portions of a document relevant to the user query.\n",
    "\n",
    "User questions will begin with the token: ###Question.\n",
    "\n",
    "Please answer only using the context provided in the input. Do not mention anything about the context in your final answer.\n",
    "\n",
    "If the answer is not found in the context, respond \"I don't know\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "7rY0nLlukWsp"
   },
   "outputs": [],
   "source": [
    "qna_user_message_template = \"\"\"\n",
    "###Context\n",
    "Here are some documents that are relevant to the question mentioned below.\n",
    "{context}\n",
    "\n",
    "###Question\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EBk1o34nxio"
   },
   "source": [
    "## Response Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "VIS9YhrqkZb5"
   },
   "outputs": [],
   "source": [
    "def generate_rag_response(user_input,k=3,max_tokens=128,temperature=0,top_p=0.95,top_k=50):\n",
    "    global qna_system_message,qna_user_message_template\n",
    "    # Retrieve relevant document chunks\n",
    "    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=k)\n",
    "    context_list = [d.page_content for d in relevant_document_chunks]\n",
    "\n",
    "    # Combine document chunks into a single context\n",
    "    context_for_query = \". \".join(context_list)\n",
    "\n",
    "    user_message = qna_user_message_template.replace('{context}', context_for_query)\n",
    "    user_message = user_message.replace('{question}', user_input)\n",
    "\n",
    "    prompt = qna_system_message + '\\n' + user_message\n",
    "\n",
    "    # Generate the response\n",
    "    try:\n",
    "        response = llm(\n",
    "                  prompt=prompt,\n",
    "                  max_tokens=max_tokens,\n",
    "                  temperature=temperature,\n",
    "                  top_p=top_p,\n",
    "                  top_k=top_k\n",
    "                  )\n",
    "\n",
    "        # Extract and print the model's response\n",
    "        response = response['choices'][0]['text'].strip()\n",
    "    except Exception as e:\n",
    "        response = f'Sorry, I encountered the following error: \\n {e}'\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_ZFe7Xkn2bn"
   },
   "source": [
    "# Question Answering using RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMSrD6qmn4ex"
   },
   "source": [
    "### Query 1: Who are the authors of this article and who published this article ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "fL6HbmtrkdJI",
    "outputId": "75c00a91-2132-4e0d-a542-af83456d05cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\nWithout additional context or information, it is impossible for me to determine the'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using only LLM\n",
    "llm(\"Who are the authors of this article and who published this article ?\")['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7wgpBfwmJ3T",
    "outputId": "fb7d25ba-a08d-4df8-c710-6ce4536df03d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Morten T. Hansen and Joel M. Podolny are the authors of the article. Harvard Business Review published it.\n"
     ]
    }
   ],
   "source": [
    "# Using RAG\n",
    "user_input = \"Who are the authors of this article and who published this article ?\"\n",
    "print(generate_rag_response(user_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1BFaFgzoCjZ"
   },
   "source": [
    "- The answer is clear, concise, and focused, without any unnecessary information.  \n",
    "\n",
    "- For queries like this, we expect a response of this nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cSy2VYioEbb"
   },
   "source": [
    "### Query 2: List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "9RPMwwqgmNMl",
    "outputId": "81fa0bb4-3ec0-4f1b-a7b5-09850bebd725"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\n1. Visionary:\\n   - Ability to see and artic'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using only LLM\n",
    "llm(\"List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\")['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "SwXft1kzmfTK",
    "outputId": "6e863052-6c42-4bba-93ed-01550434f38a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"- Deep expertise: Apple's managers are expected to possess deep expertise that allows them to meaningfully engage in all the work being done within their individual functions. The assumption is that it's easier to train an expert to manage well than to train a manager to be an expert. At Apple, experts lead experts and this approach cascades down all levels of the organization.\\n- Immersion in the details: Leaders should know the details of their organization three levels down because that is essential for speedy and effective cross-functional decision-making at the highest levels. Managers attend decision-making meetings without the details\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using RAG\n",
    "user_input_2 = \"List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\"\n",
    "generate_rag_response(user_input_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd7eekRRoLPp"
   },
   "source": [
    "- The response contains only two leadership characteristics, but they are well explained.  \n",
    "- Perhaps if we increase the **`max_tokens`**, we might get the third characteristic as well (assuming it is in the document)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrSqLjgVoNIL"
   },
   "source": [
    "### Query 3: Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "hSc2imE7nLEK",
    "outputId": "08d44ea6-0edb-40b3-a0fa-806f06fd587b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"I don't know. The context does mention that Apple's functional organization and leadership model have played a crucial role in the company's innovation success, but it doesn't provide specific examples of how this has manifested in successful innovations.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input_3 = \"Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\"\n",
    "generate_rag_response(user_input_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMo8_UaOoSnN"
   },
   "source": [
    "- If we look at the system prompt, we explicitly mentioned that the query should not be answered if it cannot be derived from the context.  \n",
    "\n",
    "- As expected, the model has done its job well. It has eliminated hallucination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVjPBr8boVxH"
   },
   "source": [
    "## Fine-tuning Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fv3nsUOXoWwF"
   },
   "source": [
    "### Example Prompt: \"Once upon a time, in a distant forest, a young fox\"\n",
    "------------------------------------------------------\n",
    "### Step 1: Model predicts probabilities for next token\n",
    "-------------------------------------------------\n",
    "Token probs (example):\n",
    "\"sat\" -> 0.30\n",
    "\"danced\" -> 0.25\n",
    "\"ran\" -> 0.15\n",
    "\"explored\" -> 0.10\n",
    "\"jumped\" -> 0.05\n",
    "...others -> 0.15\n",
    "\n",
    "### Step 2: Apply Temperature\n",
    "-------------------------------------------------\n",
    "temperature = 0.8\n",
    "- Scales probabilities\n",
    "- Flattens or sharpens the distribution\n",
    "- Example adjusted probs:\n",
    "\"sat\" -> 0.25\n",
    "\"danced\" -> 0.22\n",
    "\"ran\" -> 0.16\n",
    "\"explored\" -> 0.13\n",
    "\"jumped\" -> 0.08\n",
    "...others -> 0.16\n",
    "\n",
    "### Step 3: Apply Top-k / Top-p\n",
    "-------------------------------------------------\n",
    "- top_k = 3 → only keep [\"sat\", \"danced\", \"ran\"]\n",
    "- top_p = 0.7 → keep smallest set of tokens whose cumulative probability ≥ 0.7\n",
    "  - cumulative probs: \"sat\"(0.25) + \"danced\"(0.22) + \"ran\"(0.16) = 0.63 → include next token \"explored\" (0.13) → total 0.76 > 0.7\n",
    "- Final candidate tokens = [\"sat\", \"danced\", \"ran\", \"explored\"]\n",
    "\n",
    "### Step 4: Sampling\n",
    "-------------------------------------------------\n",
    "- Randomly pick **next token** from candidate set\n",
    "- Probability influenced by temperature\n",
    "- Example chosen token: \"danced\"\n",
    "\n",
    "### Step 5: Repeat\n",
    "-------------------------------------------------\n",
    "- Add \"danced\" to prompt\n",
    "- Repeat steps 1–4 for next token\n",
    "- Stop after reaching **max_tokens** limit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76B7zjk5oZQv"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bj4unLlwodaI"
   },
   "source": [
    "### Query 1: Who are the authors of this article and who published this article ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "xDMXg6eenMrA",
    "outputId": "0af932e6-2a98-47a7-cf92-714511cf326f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Answer:\\nMorten T. Hansen and Joel M. Podolny are the authors of the article. Harvard Business Review published it.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = \"Who are the authors of this article and who published this article ?\"\n",
    "generate_rag_response(user_input, max_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUpgLzk7okGg"
   },
   "source": [
    "- Even if the **`max_tokens`** is set to 100, the model still didn't generate that many, as the query could be answered with a limited number of tokens.  \n",
    "\n",
    "- One of the reasons could be that the temperature is set to 0, making the model more deterministic and less creative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TexZ-M-opqD"
   },
   "source": [
    "### Query 2: List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "ftK4wijuorJR",
    "outputId": "5f7c18f5-c51f-464d-d01e-56c6a8bb9904"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"- Deep expertise: Apple's managers are expected to possess deep expertise that allows them to meaningfully engage in all the work being done within their individual functions. The assumption is that it's easier to train an expert to manage well than to train a manager to be an expert. At Apple, experts lead experts and this approach cascades down all levels of the organization.\\n- Immersion in the details: Leaders at Apple are expected to know the details of their organization three levels down for effective cross-functional decision-making at the highest levels. Managers attend decision-making meetings with the details at their disposal, and if not, the decision must either be made without the details or postponed. Apple's leaders pay extreme attention to the exact shape of products' rounded corners and demand extremely precise manufacturing tolerances to produce millions of products with continuous curves.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input_2 = \"List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\"\n",
    "generate_rag_response(user_input_2, temperature=0.1, max_tokens=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7FATUU0otqs"
   },
   "source": [
    "- If we compare it to the previous case, after increasing the **`max_tokens`**, we got the third characteristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ijEtyyAov4o"
   },
   "source": [
    "### Query 3: Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "pxi5sGCWo5xK",
    "outputId": "d7cb2f3b-d0ce-4f81-f190-2d07e3eb283e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"I don't know. The context does mention that Apple's functional organization and leadership model have played a crucial role in the company's innovation success, but it doesn't provide specific examples of how this has manifested in successful innovations.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input_3 = \"Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\"\n",
    "generate_rag_response(user_input_3, top_p=0.98, top_k=20, max_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHAlAK52pAON"
   },
   "source": [
    "- Since the context provided doesn't help with the query, the model has responded correctly based on the prompt design.  \n",
    "\n",
    "- However, there is a chance that it might not be present in the top **`k`** context. Therefore, it is better to experiment with higher values of **`k`** and check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRGQPUCHpE-p"
   },
   "source": [
    "# Output Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXDWyj7spGKh"
   },
   "source": [
    "Let us now use the LLM-as-a-judge method to check the quality of the RAG system on two parameters - retrieval and generation. We illustrate this evaluation based on the answeres generated to the question from the previous section.\n",
    "\n",
    "- We are using the same Mistral model for evaluation, so basically here the llm is rating itself on how well he has performed in the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vv6kWYhEpLUZ"
   },
   "source": [
    "### Defining the Evaluation Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "RPW8qoV1pNgQ"
   },
   "outputs": [],
   "source": [
    "groundedness_rater_system_message = \"\"\"\n",
    "You are tasked with rating AI generated answers to questions posed by users.\n",
    "You will be presented a question, context used by the AI system to generate the answer and an AI generated answer to the question.\n",
    "In the input, the question will begin with ###Question, the context will begin with ###Context while the AI generated answer will begin with ###Answer.\n",
    "\n",
    "Evaluation criteria:\n",
    "The task is to judge the extent to which the metric is followed by the answer.\n",
    "1 - The metric is not followed at all\n",
    "2 - The metric is followed only to a limited extent\n",
    "3 - The metric is followed to a good extent\n",
    "4 - The metric is followed mostly\n",
    "5 - The metric is followed completely\n",
    "\n",
    "Metric:\n",
    "The answer should be derived only from the information presented in the context\n",
    "\n",
    "Instructions:\n",
    "1. First write down the steps that are needed to evaluate the answer as per the metric.\n",
    "2. Give a step-by-step explanation if the answer adheres to the metric considering the question and context as the input.\n",
    "3. Next, evaluate the extent to which the metric is followed.\n",
    "4. Use the previous information to rate the answer using the evaluaton criteria and assign a score.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "UZkXEj4zpQtI"
   },
   "outputs": [],
   "source": [
    "relevance_rater_system_message = \"\"\"\n",
    "You are tasked with rating AI generated answers to questions posed by users.\n",
    "You will be presented a question, context used by the AI system to generate the answer and an AI generated answer to the question.\n",
    "In the input, the question will begin with ###Question, the context will begin with ###Context while the AI generated answer will begin with ###Answer.\n",
    "\n",
    "Evaluation criteria:\n",
    "The task is to judge the extent to which the metric is followed by the answer.\n",
    "1 - The metric is not followed at all\n",
    "2 - The metric is followed only to a limited extent\n",
    "3 - The metric is followed to a good extent\n",
    "4 - The metric is followed mostly\n",
    "5 - The metric is followed completely\n",
    "\n",
    "Metric:\n",
    "Relevance measures how well the answer addresses the main aspects of the question, based on the context.\n",
    "Consider whether all and only the important aspects are contained in the answer when evaluating relevance.\n",
    "\n",
    "Instructions:\n",
    "1. First write down the steps that are needed to evaluate the context as per the metric.\n",
    "2. Give a step-by-step explanation if the context adheres to the metric considering the question as the input.\n",
    "3. Next, evaluate the extent to which the metric is followed.\n",
    "4. Use the previous information to rate the context using the evaluaton criteria and assign a score.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "gOKP_wCOpTke"
   },
   "outputs": [],
   "source": [
    "user_message_template = \"\"\"\n",
    "###Question\n",
    "{question}\n",
    "\n",
    "###Context\n",
    "{context}\n",
    "\n",
    "###Answer\n",
    "{answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APgQPCfwpZmH"
   },
   "source": [
    "### Defining the Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "s702d30upWGN"
   },
   "outputs": [],
   "source": [
    "def generate_ground_relevance_response(user_input,k=3,max_tokens=128,temperature=0,top_p=0.95,top_k=50):\n",
    "    global qna_system_message,qna_user_message_template\n",
    "    # Retrieve relevant document chunks\n",
    "    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=3)\n",
    "    context_list = [d.page_content for d in relevant_document_chunks]\n",
    "    context_for_query = \". \".join(context_list)\n",
    "\n",
    "    # Combine user_prompt and system_message to create the prompt\n",
    "    prompt = f\"\"\"[INST]{qna_system_message}\\n\n",
    "                {'user'}: {qna_user_message_template.format(context=context_for_query, question=user_input)}\n",
    "                [/INST]\"\"\"\n",
    "\n",
    "    response = llm(\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            stop=['INST'],\n",
    "            )\n",
    "\n",
    "    answer =  response[\"choices\"][0][\"text\"]\n",
    "\n",
    "    # Combine user_prompt and system_message to create the prompt\n",
    "    groundedness_prompt = f\"\"\"[INST]{groundedness_rater_system_message}\\n\n",
    "                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}\n",
    "                [/INST]\"\"\"\n",
    "\n",
    "    # Combine user_prompt and system_message to create the prompt\n",
    "    relevance_prompt = f\"\"\"[INST]{relevance_rater_system_message}\\n\n",
    "                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}\n",
    "                [/INST]\"\"\"\n",
    "\n",
    "    response_1 = llm(\n",
    "            prompt=groundedness_prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            stop=['INST'],\n",
    "            )\n",
    "\n",
    "    response_2 = llm(\n",
    "            prompt=relevance_prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            stop=['INST'],\n",
    "            )\n",
    "\n",
    "    return response_1['choices'][0]['text'],response_2['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ANbR96cpfpc"
   },
   "source": [
    "### Query 1: Who are the authors of this article and who published this article ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3iOdrbepgqy",
    "outputId": "33d80b39-2da9-46f5-c032-c6627776d696"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Steps to evaluate the answer:\n",
      "1. Identify the key information in the context related to the question.\n",
      "2. Check if the answer is derived only from the identified information in the context.\n",
      "3. Evaluate the extent to which the metric is followed.\n",
      "\n",
      "Explanation:\n",
      "The question asks for the authors of the article and the publisher. The context provides the names of the authors (Morten T. Hansen and Joel M. Podolny) and the name of the publisher (Harvard Business Review). The answer correctly identifies both the authors and the publisher from the information given in the context. Therefore, the answer is derived only from the information presented in the context.\n",
      "\n",
      "Evaluation:\n",
      "The metric is followed completely as the answer is derived solely from the context without any additional information or assumptions.\n",
      "\n",
      "Rating:\n",
      "Based on the evaluation criteria, I would rate the answer a 5 for following the metric completely.\n",
      "\n",
      " Steps to evaluate the context as per the metric:\n",
      "1. Identify the main aspects of the question: In this case, the main aspects of the question are identifying the authors and the publisher of the article.\n",
      "2. Determine if the context contains all and only the important aspects: The context provides the names of the authors (Morten T. Hansen and Joel M. Podolny) and the name of the publisher (Harvard Business Review). Therefore, it adheres to the metric as it contains all the necessary information to answer the question.\n",
      "\n",
      "The extent to which the metric is followed:\n",
      "The context follows the metric completely as it provides all the important aspects of the question in the answer.\n",
      "\n",
      "Rating the context using the evaluation criteria and assigning a score:\n",
      "Since the context follows the metric completely, I would rate it a 5 on the evaluation criteria scale.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Who are the authors of this article and who published this article ?\"\n",
    "ground,rel = generate_ground_relevance_response(user_input,max_tokens=350)\n",
    "\n",
    "print(ground,end=\"\\n\\n\")\n",
    "print(rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6G31ULywpmHL"
   },
   "source": [
    "- It got a perfect score because the response is both grounded in the context and relevant to the query.  \n",
    "- This means that both the retrieval and augmentation parts are good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HN6VOpbsppro"
   },
   "source": [
    "### Query 2: List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vXxA3d0Rpq4a",
    "outputId": "b99fec90-54a9-430e-c00c-cd1b74948583"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Steps to evaluate the answer:\n",
      "1. Identify the leadership characteristics mentioned in the question and context.\n",
      "2. Determine if each line of the AI generated answer is derived directly from the information presented in the context.\n",
      "3. Check if the explanation for each characteristic adheres to the metric by ensuring that it only uses information from the context.\n",
      "\n",
      "The first characteristic, \"Deep expertise,\" is explained as Apple's managers being expected to possess deep expertise in their individual functions and experts leading other experts. This directly aligns with the context which states, \"Apple’s managers at every level, from senior vice president on down, have been expected to possess three key leadership characteristics:\n",
      "\n",
      " Steps to evaluate context as per relevance metric:\n",
      "1. Identify the main aspects of the question: In this case, the question asks for three leadership characteristics at Apple and an explanation of each one under two lines.\n",
      "2. Determine if the context provides information on the main aspects: The context discusses Apple's functional organization and the leadership model underlying it, specifically focusing on the three leadership characteristics: deep expertise and immersion in the details.\n",
      "3. Check if the context explains each characteristic: The context not only lists the characteristics but also provides an\n"
     ]
    }
   ],
   "source": [
    "user_input_2 = \"List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\"\n",
    "ground,rel = generate_ground_relevance_response(user_input_2,max_tokens=500)\n",
    "\n",
    "print(ground,end=\"\\n\\n\")\n",
    "print(rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcS127Aup1M0"
   },
   "source": [
    "- The groundedness score is 5 since the response is derived solely from the context.  \n",
    "\n",
    "- Regarding relevance, the score is 4 (the metric is mostly followed). However, it is not very clear why this rating was given.  \n",
    "\n",
    "- One solution is to modify the relevance prompt to instruct the model to provide reasons for any point deductions or increase the max_tokens (assuming the output has been truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-NHq3AWp3Qk"
   },
   "source": [
    "### Query 3: Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjZ2yu1ip4aQ",
    "outputId": "67a6f05c-f3d4-43ca-db35-135f4bba6ff8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Steps to evaluate the answer:\n",
      "1. Identify the specific examples mentioned in the article regarding Apple's approach to leadership leading to successful innovations.\n",
      "\n",
      " Steps to evaluate context\n"
     ]
    }
   ],
   "source": [
    "user_input_3 = \"Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\"\n",
    "ground,rel = generate_ground_relevance_response(user_input_3,max_tokens=500)\n",
    "\n",
    "print(ground,end=\"\\n\\n\")\n",
    "print(rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNUjqlPIp8uo"
   },
   "source": [
    "- For relevance, the response includes both the score and the reason for the point deduction.  \n",
    "\n",
    "- For groundedness, it is unclear why one point was deducted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyxRv_nMqAi4"
   },
   "source": [
    "# Business Insights and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZj8gf64qBb8"
   },
   "source": [
    "- Vector database creation time increases with the number of pages in the PDF document.\n",
    "- Retrieval parameter **`k`** is critical as the answer can be spread across multiple contexts.\n",
    "- **`chunk_overlap`** ensures coherence, especially when context spans across chunks.\n",
    "- **`max_tokens`** depends on query complexity; higher values yield detailed responses, while simple queries result in concise outputs despite large token limits due to prompt design and zero **`temperature`**.\n",
    "- Refine prompt design and temperature settings to control response length and creativity.\n",
    "- Continuously adjust RAG parameters based on specific use cases for optimal performance.\n",
    "- Prioritize groundedness and relevance in evaluations to ensure reliable and contextually accurate outputs.\n",
    "- Establish a feedback loop to fine-tune parameters, improving performance for diverse query types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r57-4iEgum5X"
   },
   "source": [
    "<font size=6 color='blue'>Thanks...</font>\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
